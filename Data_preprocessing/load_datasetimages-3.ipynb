{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.load('/Users/anmolchalise/Desktop/affectnet/train_set/annotations/train_val.npz')\n",
    "aro = np.load('/Users/anmolchalise/Desktop/affectnet/train_set/annotations/train_aro.npz')\n",
    "exp = np.load('/Users/anmolchalise/Desktop/affectnet/train_set/annotations/train_exp.npz')\n",
    "lnd = np.load('/Users/anmolchalise/Desktop/affectnet/train_set/annotations/train_lnd.npz')\n",
    "img_path = np.load('/Users/anmolchalise/Desktop/affectnet/train_set/annotations/train_imgpath.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData = pd.DataFrame(img_path['path'],columns=['image'])\n",
    "dfData.insert(1, \"Valence\", val['val'])\n",
    "dfData.insert(2, \"Arousal\", aro['aro'])\n",
    "dfData.insert(3, \"Expression Class\", exp['exp'])\n",
    "dfData['Landmarks'] = [row.tolist() for row in lnd['lnd']]\n",
    "dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_grp_exp = dfData.groupby('Expression Class')\n",
    "dfData_grp_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral =  74874\n",
      "Happy =  134415\n",
      "Sad =  25459\n",
      "Surprise =  14090\n",
      "Fear =  6378\n",
      "Disgust =  3803\n",
      "Anger =  24882\n",
      "Contempt =  3750\n"
     ]
    }
   ],
   "source": [
    "dfData_neu = dfData_grp_exp.get_group('0')\n",
    "dfData_hap = dfData_grp_exp.get_group('1')\n",
    "dfData_sad = dfData_grp_exp.get_group('2')\n",
    "dfData_sur = dfData_grp_exp.get_group('3')\n",
    "dfData_fea = dfData_grp_exp.get_group('4')\n",
    "dfData_dis = dfData_grp_exp.get_group('5')\n",
    "dfData_ang = dfData_grp_exp.get_group('6')\n",
    "dfData_con = dfData_grp_exp.get_group('7')\n",
    "print('Neutral = ',len(dfData_neu))\n",
    "print('Happy = ',len(dfData_hap))\n",
    "print('Sad = ',len(dfData_sad))\n",
    "print('Surprise = ',len(dfData_sur))\n",
    "print('Fear = ',len(dfData_fea))\n",
    "print('Disgust = ',len(dfData_dis))\n",
    "print('Anger = ',len(dfData_ang))\n",
    "print('Contempt = ',len(dfData_con))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Sampling Based On Emoset Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "dfData_neu_smp = dfData_neu[:15037]\n",
    "dfData_hap_smp = dfData_hap[:19445]\n",
    "dfData_sad_smp = dfData_sad[:12676]\n",
    "dfData_sur_smp = dfData_sur[:14090]\n",
    "dfData_fea_smp = dfData_fea[:10078]\n",
    "# dfData_dis_smp = dfData_dis[:3803]\n",
    "# dfData_ang_smp = dfData_ang[:10660]\n",
    "# dfData_con_smp = dfData_con[:3750]\n",
    "\n",
    "print('Neutral = ',len(dfData_neu_smp))\n",
    "print('Happy = ',len(dfData_hap_smp))\n",
    "print('Sad = ',len(dfData_sad_smp))\n",
    "print('Surprise = ',len(dfData_sur_smp))\n",
    "print('Fear = ',len(dfData_fea_smp))\n",
    "# print('Disgust = ',len(dfData_dis_smp))\n",
    "# print('Anger = ',len(dfData_ang_smp))\n",
    "# print('Contempt = ',len(dfData_con_smp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Expression Class</th>\n",
       "      <th>Landmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267194</th>\n",
       "      <td>train_set/images/73478.jpg</td>\n",
       "      <td>0.396825</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>3</td>\n",
       "      <td>[41.94991869918698, 90.17626556016599, 40.6113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253714</th>\n",
       "      <td>train_set/images/55952.jpg</td>\n",
       "      <td>0.163906</td>\n",
       "      <td>0.733264</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.922737306843268, 61.59569230769232, 3.34269...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37581</th>\n",
       "      <td>train_set/images/149006.jpg</td>\n",
       "      <td>0.808169</td>\n",
       "      <td>0.120983</td>\n",
       "      <td>1</td>\n",
       "      <td>[22.11190751445087, 82.6231048951049, 23.50381...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>train_set/images/101539.jpg</td>\n",
       "      <td>0.458281</td>\n",
       "      <td>0.0579435</td>\n",
       "      <td>1</td>\n",
       "      <td>[12.977963446475195, 86.59904306220096, 16.545...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>train_set/images/105362.jpg</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[25.44410256410256, 57.23333333333333, 18.2358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41411</th>\n",
       "      <td>train_set/images/153957.jpg</td>\n",
       "      <td>0.912698</td>\n",
       "      <td>0.15873</td>\n",
       "      <td>1</td>\n",
       "      <td>[23.779915611814346, 108.95662447257384, 24.22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167219</th>\n",
       "      <td>train_set/images/31749.jpg</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>3</td>\n",
       "      <td>[48.61879518072289, 69.61818181818182, 42.1146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35640</th>\n",
       "      <td>train_set/images/14652.jpg</td>\n",
       "      <td>-0.0992063</td>\n",
       "      <td>0.789337</td>\n",
       "      <td>4</td>\n",
       "      <td>[3.1414634146341474, 129.2567597765363, 4.2614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93461</th>\n",
       "      <td>train_set/images/221707.jpg</td>\n",
       "      <td>-0.07259</td>\n",
       "      <td>0.890437</td>\n",
       "      <td>4</td>\n",
       "      <td>[23.878400000000003, 73.85128834355828, 22.833...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273567</th>\n",
       "      <td>train_set/images/81651.jpg</td>\n",
       "      <td>-0.103175</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>4</td>\n",
       "      <td>[-19.776711111111112, 122.17875268817205, -17....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71326 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image     Valence    Arousal Expression Class  \\\n",
       "267194   train_set/images/73478.jpg    0.396825   0.857143                3   \n",
       "253714   train_set/images/55952.jpg    0.163906   0.733264                3   \n",
       "37581   train_set/images/149006.jpg    0.808169   0.120983                1   \n",
       "1200    train_set/images/101539.jpg    0.458281  0.0579435                1   \n",
       "4083    train_set/images/105362.jpg    0.650794          0                1   \n",
       "...                             ...         ...        ...              ...   \n",
       "41411   train_set/images/153957.jpg    0.912698    0.15873                1   \n",
       "167219   train_set/images/31749.jpg    0.174603   0.769841                3   \n",
       "35640    train_set/images/14652.jpg  -0.0992063   0.789337                4   \n",
       "93461   train_set/images/221707.jpg    -0.07259   0.890437                4   \n",
       "273567   train_set/images/81651.jpg   -0.103175   0.563492                4   \n",
       "\n",
       "                                                Landmarks  \n",
       "267194  [41.94991869918698, 90.17626556016599, 40.6113...  \n",
       "253714  [6.922737306843268, 61.59569230769232, 3.34269...  \n",
       "37581   [22.11190751445087, 82.6231048951049, 23.50381...  \n",
       "1200    [12.977963446475195, 86.59904306220096, 16.545...  \n",
       "4083    [25.44410256410256, 57.23333333333333, 18.2358...  \n",
       "...                                                   ...  \n",
       "41411   [23.779915611814346, 108.95662447257384, 24.22...  \n",
       "167219  [48.61879518072289, 69.61818181818182, 42.1146...  \n",
       "35640   [3.1414634146341474, 129.2567597765363, 4.2614...  \n",
       "93461   [23.878400000000003, 73.85128834355828, 22.833...  \n",
       "273567  [-19.776711111111112, 122.17875268817205, -17....  \n",
       "\n",
       "[71326 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfData_smp = pd.concat([dfData_neu_smp, dfData_hap_smp, dfData_sad_smp, dfData_sur_smp, dfData_fea_smp])\n",
    "dfData_smp = dfData_smp.sample(frac=1)\n",
    "dfData_smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving downsampled .npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath_downsmp = dfData_smp['image'].to_numpy()\n",
    "np.savez('/Users/anmolchalise/Desktop/affectnet/train_set/annotations_5/train_imgpath_smp.npz', path=imgpath_downsmp)\n",
    "exp_downsmp = dfData_smp['Expression Class'].to_numpy()\n",
    "np.savez('/Users/anmolchalise/Desktop/affectnet/train_set/annotations_5/train_exp_smp.npz', exp=exp_downsmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_downsmp = dfData_smp['Valence'].to_numpy()\n",
    "np.savez('/Users/anmolchalise/Desktop/affectnet/train_set/annotations_5/train_val_smp.npz', val=val_downsmp)\n",
    "aro_downsmp = dfData_smp['Arousal'].to_numpy()\n",
    "np.savez('/Users/anmolchalise/Desktop/affectnet/train_set/annotations_5/train_aro_smp.npz', aro=aro_downsmp)\n",
    "lnd_downsmp = dfData_smp['Landmarks'].to_numpy()\n",
    "np.savez('/Users/anmolchalise/Desktop/affectnet/train_set/annotations_5/train_lnd_smp.npz', lnd=lnd_downsmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking whether downsampling is successful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71326,)\n",
      "['train_set/images/73478.jpg' 'train_set/images/55952.jpg'\n",
      " 'train_set/images/149006.jpg' ... 'train_set/images/14652.jpg'\n",
      " 'train_set/images/221707.jpg' 'train_set/images/81651.jpg']\n",
      "(71326,)\n",
      "['3' '3' '1' ... '4' '4' '4']\n"
     ]
    }
   ],
   "source": [
    "imgpath_load = np.load('/Users/anmolchalise/Desktop/affectnet/train_set/annotations_5/train_imgpath_smp.npz', allow_pickle=True)\n",
    "exp_load = np.load('/Users/anmolchalise/Desktop/affectnet/train_set/annotations_5/train_exp_smp.npz', allow_pickle=True)\n",
    "\n",
    "print(imgpath_load['path'].shape)\n",
    "print(imgpath_load['path'])\n",
    "print(exp_load['exp'])\n",
    "\n",
    "print(exp_load['exp'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.load('/Users/anmolchalise/Desktop/affectnet/val_set/annotations/val_val.npz')\n",
    "aro = np.load('/Users/anmolchalise/Desktop/affectnet/val_set/annotations/val_aro.npz')\n",
    "exp = np.load('/Users/anmolchalise/Desktop/affectnet/val_set/annotations/val_exp.npz')\n",
    "lnd = np.load('/Users/anmolchalise/Desktop/affectnet/val_set/annotations/val_lnd.npz')\n",
    "img_path = np.load('/Users/anmolchalise/Desktop/affectnet/val_set/annotations/val_img_path.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/.DS_Store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/100.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>/992.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>/994.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>/995.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>/996.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>/999.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           image\n",
       "0     /.DS_Store\n",
       "1         /0.jpg\n",
       "2         /1.jpg\n",
       "3        /10.jpg\n",
       "4       /100.jpg\n",
       "...          ...\n",
       "3995    /992.jpg\n",
       "3996    /994.jpg\n",
       "3997    /995.jpg\n",
       "3998    /996.jpg\n",
       "3999    /999.jpg\n",
       "\n",
       "[4000 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfData = pd.DataFrame(img_path['img_path'],columns=['image'])\n",
    "# dfData.insert(1, \"Valence\", val['val'])\n",
    "# dfData.insert(2, \"Arousal\", aro['aro'])\n",
    "# dfData.insert(3, \"Expression Class\", exp['exp'])\n",
    "# dfData['Landmarks'] = [row.tolist() for row in lnd['lnd']]\n",
    "# dfData.head()\n",
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
